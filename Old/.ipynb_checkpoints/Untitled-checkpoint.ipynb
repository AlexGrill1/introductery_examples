{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import itertools \n",
    "import matplotlib \n",
    "import matplotlib.style \n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd \n",
    "import sys \n",
    "import os\n",
    "  \n",
    "from collections import defaultdict \n",
    "#import plotting \n",
    "  \n",
    "matplotlib.style.use('ggplot') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnregisteredEnv",
     "evalue": "No registered env with id: bs_env-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mspec\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bs_env-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnregisteredEnv\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d773709a16dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bs_env-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Making new env: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mspec\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No registered env with id: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnregisteredEnv\u001b[0m: No registered env with id: bs_env-v0"
     ]
    }
   ],
   "source": [
    "gym.make('bs_env-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSEnv:\n",
    "    '''A simple discrete time BS environment'''\n",
    "    \n",
    "    def __init__(self, mu, sigma, r, T, delta_t, V_0=100): #S_0=100\n",
    "        #self.S_0     = S_0      # initial stock price\n",
    "        self.mu      = mu       # expected stock return\n",
    "        self.sigma   = sigma    # stock standard deviation\n",
    "        self.r       = r        # risk-free interest rate\n",
    "        self.T       = T        # investment horizon\n",
    "        self.delta_t = delta_t  # time-step size\n",
    "        self.V_0     = V_0\n",
    "        self.action_space = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])   #investment in stock\n",
    "        self.n_actions = len(self.action_space)\n",
    "        \n",
    "        self.t      = 0         # time\n",
    "        self.V_t    = V_0       # initial wealth\n",
    "        self.state  = (0, V_0)  # initial state\n",
    "        self.reward = 0         # initial reward\n",
    "        \n",
    "        self.wealth_bins = None\n",
    "        \n",
    "        \n",
    "    #def sample_trajectory(self):\n",
    "    #    returns      = np.random.normal(loc=self.delta_t * self.mu, scale=math.sqrt(self.delta_t) * self.sigma, size=math.ceil(self.T / self.delta_t))\n",
    "    #    returns      = np.insert(returns, 0, 0)\n",
    "    #    stock_prices = self.S_0 * np.cumprod(1+returns)\n",
    "    #    return(stock_prices, returns)\n",
    "    \n",
    "    \n",
    "    def rewards_from_prices(self, stock_prices, utility = \"log\"):\n",
    "        rewards = np.zeros_like(stock_prices)        # all rewards until last time-step are zero\n",
    "        if utility == \"log\":\n",
    "            rewards[-1] = np.log(stock_prices[-1])   # last reward R_T = U(S_T)\n",
    "        else:\n",
    "            raise ValueError\\\n",
    "            ('utility function must be one of the following: log')\n",
    "        return(rewards)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        '''Resets the environment to the state (t=0, V_0)'''\n",
    "        \n",
    "        self.t = 0\n",
    "        self.state = (0, self.V_0)\n",
    "        self.reward = 0\n",
    "        return(self.state)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        '''Computes one step in BS Environment.'''\n",
    "        \n",
    "        self.t    += self.delta_t                   # Update time\n",
    "        self.V_t  *= action * (np.random.normal(loc=self.delta_t * self.mu, scale=math.sqrt(self.delta_t) * self.sigma) - self.r) + (1 + self.r)  # Update Wealth (see notes)\n",
    "        #self.state = (self.t, self.V_t)             # Update state\n",
    "        self.state = self.get_state(50, 150, 5)\n",
    "        \n",
    "        reward = np.log(self.V_t)*(self.t==self.T)  # Get reward according to log utility at terminal time step\n",
    "        done   = self.t==self.T                    # End of investment period\n",
    "        return(self.state, reward, done, self.t, self.V_t)\n",
    "    \n",
    "    \n",
    "    def get_state(self, lower, upper, delta_bin):\n",
    "        '''Computes the discrete state (t, V_t), for continuous t'''\n",
    "        \n",
    "        if self.wealth_bins is None:\n",
    "            # Create bins (0, lower] < (lower, lower + delta_bin] < (lower + delta_bin, lower + 2*delta_bin] < ... < (upper, inf]\n",
    "            self.wealth_bins = [0] + np.arange(lower, upper, delta_bin).tolist() + [float('Inf')] \n",
    "            \n",
    "        return((self.t, pd.cut(x=[self.V_t], bins=self.wealth_bins, labels=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "    \"\"\" \n",
    "    Creates an epsilon-greedy policy based \n",
    "    on a given Q-function and epsilon. \n",
    "       \n",
    "    Returns a function that takes the state \n",
    "    as an input and returns the probabilities \n",
    "    for each action in the form of a numpy array  \n",
    "    of length of the action space(set of possible actions). \n",
    "    \"\"\"\n",
    "    def policyFunction(state): \n",
    "   \n",
    "        Action_probabilities = np.ones(num_actions, \n",
    "                dtype = float) * epsilon / num_actions \n",
    "                  \n",
    "        best_action = np.argmax(Q[state]) \n",
    "        Action_probabilities[best_action] += (1.0 - epsilon) \n",
    "        return Action_probabilities \n",
    "   \n",
    "    return policyFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0, \n",
    "                            alpha = 0.6, epsilon = 0.1): \n",
    "    \"\"\" \n",
    "    Q-Learning algorithm: Off-policy TD control. \n",
    "    Finds the optimal greedy policy while improving \n",
    "    following an epsilon-greedy policy\"\"\"\n",
    "       \n",
    "    # Action value function \n",
    "    # A nested dictionary that maps \n",
    "    # state -> (action -> action-value). \n",
    "    Q = defaultdict(lambda: np.zeros(env.n_actions)) \n",
    "   \n",
    "    # Keeps track of useful statistics \n",
    "    #stats = plotting.EpisodeStats( \n",
    "    #    episode_lengths = np.zeros(num_episodes), \n",
    "    #    episode_rewards = np.zeros(num_episodes))     \n",
    "      \n",
    "    # Create an epsilon greedy policy function \n",
    "    # appropriately for environment action space \n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.n_actions) \n",
    "       \n",
    "    # For every episode \n",
    "    for ith_episode in range(num_episodes): \n",
    "           \n",
    "        # Reset the environment and pick the first action \n",
    "        state = env.reset() \n",
    "           \n",
    "        for t in itertools.count(): \n",
    "               \n",
    "            # get probabilities of all actions from current state \n",
    "            action_probabilities = policy(state) \n",
    "   \n",
    "            # choose action according to  \n",
    "            # the probability distribution \n",
    "            action = np.random.choice(env.action_space, \n",
    "                                      p = action_probabilities) \n",
    "   \n",
    "            # take action and get reward, transit to next state \n",
    "            next_state, reward, done, _, _ = env.step(action) \n",
    "   \n",
    "            # Update statistics \n",
    "            #stats.episode_rewards[ith_episode] += reward \n",
    "            #stats.episode_lengths[ith_episode] = t \n",
    "               \n",
    "            # TD Update \n",
    "            best_next_action = np.argmax(Q[next_state])     \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action] \n",
    "            td_delta = td_target - Q[state][action] \n",
    "            Q[state][action] += alpha * td_delta \n",
    "   \n",
    "            # done is True if episode terminated    \n",
    "            if done: \n",
    "                break\n",
    "                   \n",
    "            state = next_state \n",
    "       \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-23373aee2f17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBSEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-5641b741e665>\u001b[0m in \u001b[0;36mqLearning\u001b[1;34m(env, num_episodes, discount_factor, alpha, epsilon)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mbest_next_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mtd_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_next_action\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mtd_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtd_target\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtd_delta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "env = BSEnv(mu=0.05, sigma=0.2, r=0.01, T=5, delta_t=0.5, V_0=100)\n",
    "\n",
    "Q, stats = qLearning(env, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "from gym import spaces\n",
    "\n",
    "class BSEnv(gym.Env):\n",
    "    '''Custom discrete-time Black-Scholes environment'''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, mu, sigma):\n",
    "        super().__init__()\n",
    "        self.mu    = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Actions\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
