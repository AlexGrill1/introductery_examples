{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gym\n",
    "from envs.binomial_tree2 import BinomialTree, decode_action    # custom BinomialTree dynamics\n",
    "from envs import plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions (Investment in risky asset): [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "actions = np.arange(0, 1.01, step=0.1)                  #vector of actions, discrete investment decisions in 10% steps\n",
    "#actions = np.array([0, 13/19, 1])\n",
    "print(\"Actions (Investment in risky asset):\", actions)  \n",
    "lower = 90                                              # upper limit of lowest wealth bin [0, lower)\n",
    "upper = 110                                             # lower limit of highest wealth bin [upper, +Inf)\n",
    "delta_bin = 20                                          # wealth-bin width\n",
    "wealth_bins = [0] + np.arange(lower, upper+1, delta_bin).tolist() + [float('Inf')]  # +1 as upper limit is not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of BinomialTree dynmics\n",
    "print(env.reset())\n",
    "print(env.V_t)\n",
    "print(env.step(0))\n",
    "print(env.V_t)\n",
    "print(env.step(10))\n",
    "print(env.V_t)\n",
    "print(env.step(10))\n",
    "print(env.V_t)\n",
    "print(env.step(10))\n",
    "print(env.V_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon-Greedy Policy**\\\n",
    "Source: https://www.geeksforgeeks.org/q-learning-in-python/#:~:text=Q%2DLearning%20is%20a%20basic,defined%20for%20states%20and%20actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "    \"\"\" \n",
    "    Creates an epsilon-greedy policy based \n",
    "    on a given Q-function and epsilon. \n",
    "       \n",
    "    Returns a function that takes the state \n",
    "    as an input and returns the probabilities \n",
    "    for each action in the form of a numpy array  \n",
    "    of length of the action space(set of possible actions). \n",
    "    \"\"\"\n",
    "    def policyFunction(state): \n",
    "   \n",
    "        Action_probabilities = np.ones(num_actions, dtype = float) * epsilon / num_actions \n",
    "        best_action = np.argmax(Q[state]) \n",
    "        Action_probabilities[best_action] += (1.0 - epsilon) \n",
    "        return Action_probabilities \n",
    "   \n",
    "    return policyFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning Algorithm** \\\n",
    "Source: https://www.geeksforgeeks.org/q-learning-in-python/#:~:text=Q%2DLearning%20is%20a%20basic,defined%20for%20states%20and%20actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1, alpha = 0.1, epsilon = 1): \n",
    "    \"\"\" \n",
    "    Q-Learning algorithm: Off-policy TD control. \n",
    "    Finds the optimal greedy policy while improving \n",
    "    following an epsilon-greedy policy\"\"\"\n",
    "       \n",
    "    # Action value function \n",
    "    # A nested dictionary that maps \n",
    "    # state -> (action -> action-value). \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) \n",
    "    A = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "   \n",
    "    # Keeps track of useful statistics \n",
    "    stats = plotting.EpisodeStats( \n",
    "        episode_lengths = np.zeros(num_episodes), \n",
    "        episode_rewards = np.zeros(num_episodes))     \n",
    "       \n",
    "    # Create an epsilon greedy policy function \n",
    "    # appropriately for environment action space \n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n) \n",
    "       \n",
    "    # For every episode\n",
    "    returns=np.array([])\n",
    "    return_ = 0\n",
    "    terminal_wealths = np.array([])\n",
    "    for ith_episode in range(num_episodes): \n",
    "           \n",
    "        # Reset the environment and pick the first action \n",
    "        state = env.reset() \n",
    "           \n",
    "        for t in itertools.count(): \n",
    "               \n",
    "            # get probabilities of all actions from current state \n",
    "            action_probabilities = policy(state)\n",
    "   \n",
    "            # choose action according to  \n",
    "            # the probability distribution \n",
    "            action = np.random.choice(np.arange( \n",
    "                      len(action_probabilities)), \n",
    "                       p = action_probabilities)\n",
    "            A[state][action] += 1\n",
    "   \n",
    "            # take action and get reward, transit to next state \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "   \n",
    "            # Update statistics \n",
    "            stats.episode_rewards[ith_episode] += reward \n",
    "            stats.episode_lengths[ith_episode] = t\n",
    "               \n",
    "            # TD Update \n",
    "            best_next_action = np.argmax(Q[next_state])     \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action] \n",
    "            td_delta = td_target - Q[state][action] \n",
    "            Q[state][action] += (1/A[state][action]) * td_delta      # Dynamic Step-Size (1/k) as in Sutton/Barto p.53\n",
    "   \n",
    "            return_ += reward\n",
    "            # done is True if episode terminated    \n",
    "            if done:\n",
    "                returns = np.append(returns, return_)\n",
    "                terminal_wealths=np.append(terminal_wealths, env.V_t)\n",
    "                return_= 0\n",
    "                break\n",
    "                   \n",
    "            state = next_state\n",
    "        \n",
    "        if (ith_episode % 10000 == 0):            \n",
    "            print(\"Episode: {}, Mean Return: {}, Mean Wealth (V_T): {}, Epsilon: {}, Alpha: {}\".format(ith_episode, round(returns.mean(), 3), round(terminal_wealths.mean(), 3), epsilon, alpha))\n",
    "            #print(\"td_delta:\", td_delta)\n",
    "            #print(Q[(0,1)])\n",
    "            #print(\"Best Action (Investment in risky asset):\", decode_action(np.argmax(Q[(0,1)]), actions))\n",
    "            returns = np.array([])\n",
    "            terminal_wealths=np.array([])\n",
    "            \n",
    "        #if (ith_episode % 100000 == 0):\n",
    "        #    #alpha = 0.0001\n",
    "        #    epsilon *= 0.7\n",
    "        #    alpha *= 0.95\n",
    "        #    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "            \n",
    "        # Epsilon-Decay    \n",
    "        #if (ith_episode % 1000 == 0) & (ith_episode != 0):\n",
    "        #    epsilon *= 0.95\n",
    "        #    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "        #    alpha = 0.1\n",
    "        \n",
    "        # Alpha-Decay\n",
    "        #if (ith_episode % 20000 == 0) & (ith_episode != 0):\n",
    "        #    if alpha > 0.00011:\n",
    "        #        alpha *= 1/10\n",
    "       \n",
    "    return Q, stats, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Mean Return: 15.492, Mean Wealth (V_T): 240.0, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 10000, Mean Return: 10.423, Mean Wealth (V_T): 118.126, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 20000, Mean Return: 10.437, Mean Wealth (V_T): 118.288, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 30000, Mean Return: 10.394, Mean Wealth (V_T): 117.22, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 40000, Mean Return: 10.424, Mean Wealth (V_T): 118.108, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 50000, Mean Return: 10.364, Mean Wealth (V_T): 116.581, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 60000, Mean Return: 10.398, Mean Wealth (V_T): 117.509, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 70000, Mean Return: 10.374, Mean Wealth (V_T): 117.028, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 80000, Mean Return: 10.391, Mean Wealth (V_T): 117.276, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 90000, Mean Return: 10.404, Mean Wealth (V_T): 117.514, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 100000, Mean Return: 10.366, Mean Wealth (V_T): 116.696, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 110000, Mean Return: 10.431, Mean Wealth (V_T): 118.041, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 120000, Mean Return: 10.365, Mean Wealth (V_T): 116.585, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 130000, Mean Return: 10.397, Mean Wealth (V_T): 117.334, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 140000, Mean Return: 10.387, Mean Wealth (V_T): 117.342, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 150000, Mean Return: 10.35, Mean Wealth (V_T): 116.377, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 160000, Mean Return: 10.349, Mean Wealth (V_T): 116.392, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 170000, Mean Return: 10.379, Mean Wealth (V_T): 117.074, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 180000, Mean Return: 10.345, Mean Wealth (V_T): 116.313, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 190000, Mean Return: 10.392, Mean Wealth (V_T): 117.112, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 200000, Mean Return: 10.428, Mean Wealth (V_T): 118.089, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 210000, Mean Return: 10.343, Mean Wealth (V_T): 116.018, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 220000, Mean Return: 10.387, Mean Wealth (V_T): 117.102, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 230000, Mean Return: 10.409, Mean Wealth (V_T): 117.589, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 240000, Mean Return: 10.382, Mean Wealth (V_T): 117.011, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 250000, Mean Return: 10.34, Mean Wealth (V_T): 115.974, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 260000, Mean Return: 10.44, Mean Wealth (V_T): 118.368, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 270000, Mean Return: 10.42, Mean Wealth (V_T): 117.9, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 280000, Mean Return: 10.362, Mean Wealth (V_T): 116.754, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 290000, Mean Return: 10.42, Mean Wealth (V_T): 118.026, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 300000, Mean Return: 10.423, Mean Wealth (V_T): 117.98, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 310000, Mean Return: 10.417, Mean Wealth (V_T): 117.982, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 320000, Mean Return: 10.39, Mean Wealth (V_T): 117.494, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 330000, Mean Return: 10.354, Mean Wealth (V_T): 116.277, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 340000, Mean Return: 10.388, Mean Wealth (V_T): 117.137, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 350000, Mean Return: 10.419, Mean Wealth (V_T): 117.882, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 360000, Mean Return: 10.346, Mean Wealth (V_T): 116.266, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 370000, Mean Return: 10.427, Mean Wealth (V_T): 118.228, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 380000, Mean Return: 10.362, Mean Wealth (V_T): 116.455, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 390000, Mean Return: 10.409, Mean Wealth (V_T): 117.56, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 400000, Mean Return: 10.422, Mean Wealth (V_T): 117.986, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 410000, Mean Return: 10.405, Mean Wealth (V_T): 117.61, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 420000, Mean Return: 10.418, Mean Wealth (V_T): 117.871, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 430000, Mean Return: 10.388, Mean Wealth (V_T): 117.202, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 440000, Mean Return: 10.364, Mean Wealth (V_T): 116.522, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 450000, Mean Return: 10.376, Mean Wealth (V_T): 116.971, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 460000, Mean Return: 10.39, Mean Wealth (V_T): 117.273, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 470000, Mean Return: 10.436, Mean Wealth (V_T): 118.23, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 480000, Mean Return: 10.373, Mean Wealth (V_T): 117.029, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 490000, Mean Return: 10.409, Mean Wealth (V_T): 117.602, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 500000, Mean Return: 10.325, Mean Wealth (V_T): 115.66, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 510000, Mean Return: 10.342, Mean Wealth (V_T): 116.059, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 520000, Mean Return: 10.385, Mean Wealth (V_T): 117.148, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 530000, Mean Return: 10.423, Mean Wealth (V_T): 117.872, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 540000, Mean Return: 10.384, Mean Wealth (V_T): 117.098, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 550000, Mean Return: 10.365, Mean Wealth (V_T): 116.394, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 560000, Mean Return: 10.417, Mean Wealth (V_T): 118.078, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 570000, Mean Return: 10.373, Mean Wealth (V_T): 116.8, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 580000, Mean Return: 10.361, Mean Wealth (V_T): 116.541, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 590000, Mean Return: 10.405, Mean Wealth (V_T): 117.581, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 600000, Mean Return: 10.353, Mean Wealth (V_T): 116.603, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 610000, Mean Return: 10.363, Mean Wealth (V_T): 116.439, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 620000, Mean Return: 10.49, Mean Wealth (V_T): 119.589, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 630000, Mean Return: 10.332, Mean Wealth (V_T): 115.774, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 640000, Mean Return: 10.35, Mean Wealth (V_T): 116.285, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 650000, Mean Return: 10.419, Mean Wealth (V_T): 117.87, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 660000, Mean Return: 10.447, Mean Wealth (V_T): 118.507, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 670000, Mean Return: 10.323, Mean Wealth (V_T): 115.752, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 680000, Mean Return: 10.408, Mean Wealth (V_T): 117.408, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 690000, Mean Return: 10.367, Mean Wealth (V_T): 116.685, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 700000, Mean Return: 10.401, Mean Wealth (V_T): 117.31, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 710000, Mean Return: 10.389, Mean Wealth (V_T): 117.207, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 720000, Mean Return: 10.45, Mean Wealth (V_T): 118.746, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 730000, Mean Return: 10.437, Mean Wealth (V_T): 118.326, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 740000, Mean Return: 10.375, Mean Wealth (V_T): 116.821, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 750000, Mean Return: 10.362, Mean Wealth (V_T): 116.546, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 760000, Mean Return: 10.436, Mean Wealth (V_T): 118.289, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 770000, Mean Return: 10.387, Mean Wealth (V_T): 117.312, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 780000, Mean Return: 10.425, Mean Wealth (V_T): 118.1, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 790000, Mean Return: 10.444, Mean Wealth (V_T): 118.452, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 800000, Mean Return: 10.333, Mean Wealth (V_T): 115.758, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 810000, Mean Return: 10.406, Mean Wealth (V_T): 117.73, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 820000, Mean Return: 10.4, Mean Wealth (V_T): 117.302, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 830000, Mean Return: 10.389, Mean Wealth (V_T): 117.215, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 840000, Mean Return: 10.368, Mean Wealth (V_T): 116.776, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 850000, Mean Return: 10.398, Mean Wealth (V_T): 117.305, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 860000, Mean Return: 10.385, Mean Wealth (V_T): 117.199, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 870000, Mean Return: 10.396, Mean Wealth (V_T): 117.292, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 880000, Mean Return: 10.43, Mean Wealth (V_T): 118.272, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 890000, Mean Return: 10.411, Mean Wealth (V_T): 117.496, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 900000, Mean Return: 10.405, Mean Wealth (V_T): 117.322, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 910000, Mean Return: 10.409, Mean Wealth (V_T): 117.744, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 920000, Mean Return: 10.432, Mean Wealth (V_T): 118.11, Epsilon: 1, Alpha: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 930000, Mean Return: 10.411, Mean Wealth (V_T): 117.643, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 940000, Mean Return: 10.376, Mean Wealth (V_T): 116.95, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 950000, Mean Return: 10.417, Mean Wealth (V_T): 117.836, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 960000, Mean Return: 10.355, Mean Wealth (V_T): 116.686, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 970000, Mean Return: 10.406, Mean Wealth (V_T): 117.613, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 980000, Mean Return: 10.382, Mean Wealth (V_T): 116.977, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 990000, Mean Return: 10.407, Mean Wealth (V_T): 117.809, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1000000, Mean Return: 10.385, Mean Wealth (V_T): 117.081, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1010000, Mean Return: 10.459, Mean Wealth (V_T): 118.935, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1020000, Mean Return: 10.414, Mean Wealth (V_T): 117.73, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1030000, Mean Return: 10.36, Mean Wealth (V_T): 116.488, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1040000, Mean Return: 10.382, Mean Wealth (V_T): 117.171, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1050000, Mean Return: 10.364, Mean Wealth (V_T): 116.599, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1060000, Mean Return: 10.373, Mean Wealth (V_T): 116.901, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1070000, Mean Return: 10.403, Mean Wealth (V_T): 117.581, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1080000, Mean Return: 10.355, Mean Wealth (V_T): 116.438, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1090000, Mean Return: 10.422, Mean Wealth (V_T): 117.737, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1100000, Mean Return: 10.383, Mean Wealth (V_T): 116.984, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1110000, Mean Return: 10.34, Mean Wealth (V_T): 116.185, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1120000, Mean Return: 10.36, Mean Wealth (V_T): 116.347, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1130000, Mean Return: 10.407, Mean Wealth (V_T): 117.745, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1140000, Mean Return: 10.396, Mean Wealth (V_T): 117.245, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1150000, Mean Return: 10.414, Mean Wealth (V_T): 117.905, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1160000, Mean Return: 10.386, Mean Wealth (V_T): 117.215, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1170000, Mean Return: 10.42, Mean Wealth (V_T): 117.975, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1180000, Mean Return: 10.405, Mean Wealth (V_T): 117.67, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1190000, Mean Return: 10.442, Mean Wealth (V_T): 118.408, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1200000, Mean Return: 10.381, Mean Wealth (V_T): 117.079, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1210000, Mean Return: 10.403, Mean Wealth (V_T): 117.582, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1220000, Mean Return: 10.401, Mean Wealth (V_T): 117.629, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1230000, Mean Return: 10.416, Mean Wealth (V_T): 117.652, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1240000, Mean Return: 10.384, Mean Wealth (V_T): 117.149, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1250000, Mean Return: 10.408, Mean Wealth (V_T): 117.736, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1260000, Mean Return: 10.394, Mean Wealth (V_T): 117.255, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1270000, Mean Return: 10.373, Mean Wealth (V_T): 116.821, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1280000, Mean Return: 10.417, Mean Wealth (V_T): 117.922, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1290000, Mean Return: 10.388, Mean Wealth (V_T): 117.297, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1300000, Mean Return: 10.423, Mean Wealth (V_T): 117.964, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1310000, Mean Return: 10.369, Mean Wealth (V_T): 116.92, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1320000, Mean Return: 10.415, Mean Wealth (V_T): 117.765, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1330000, Mean Return: 10.461, Mean Wealth (V_T): 118.645, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1340000, Mean Return: 10.344, Mean Wealth (V_T): 116.244, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1350000, Mean Return: 10.37, Mean Wealth (V_T): 116.735, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1360000, Mean Return: 10.363, Mean Wealth (V_T): 116.59, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1370000, Mean Return: 10.418, Mean Wealth (V_T): 117.875, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1380000, Mean Return: 10.392, Mean Wealth (V_T): 117.158, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1390000, Mean Return: 10.445, Mean Wealth (V_T): 118.362, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1400000, Mean Return: 10.405, Mean Wealth (V_T): 117.495, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1410000, Mean Return: 10.452, Mean Wealth (V_T): 118.945, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1420000, Mean Return: 10.443, Mean Wealth (V_T): 118.61, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1430000, Mean Return: 10.435, Mean Wealth (V_T): 118.367, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1440000, Mean Return: 10.43, Mean Wealth (V_T): 118.074, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1450000, Mean Return: 10.418, Mean Wealth (V_T): 117.922, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1460000, Mean Return: 10.459, Mean Wealth (V_T): 118.683, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1470000, Mean Return: 10.428, Mean Wealth (V_T): 117.969, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1480000, Mean Return: 10.377, Mean Wealth (V_T): 116.901, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1490000, Mean Return: 10.36, Mean Wealth (V_T): 116.621, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1500000, Mean Return: 10.355, Mean Wealth (V_T): 116.196, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1510000, Mean Return: 10.432, Mean Wealth (V_T): 118.182, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1520000, Mean Return: 10.382, Mean Wealth (V_T): 117.002, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1530000, Mean Return: 10.388, Mean Wealth (V_T): 117.169, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1540000, Mean Return: 10.427, Mean Wealth (V_T): 117.647, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1550000, Mean Return: 10.415, Mean Wealth (V_T): 117.796, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1560000, Mean Return: 10.392, Mean Wealth (V_T): 117.169, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1570000, Mean Return: 10.395, Mean Wealth (V_T): 117.081, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1580000, Mean Return: 10.37, Mean Wealth (V_T): 116.977, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1590000, Mean Return: 10.434, Mean Wealth (V_T): 118.286, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1600000, Mean Return: 10.34, Mean Wealth (V_T): 115.907, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1610000, Mean Return: 10.385, Mean Wealth (V_T): 116.869, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1620000, Mean Return: 10.373, Mean Wealth (V_T): 116.817, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1630000, Mean Return: 10.36, Mean Wealth (V_T): 116.508, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1640000, Mean Return: 10.367, Mean Wealth (V_T): 116.656, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1650000, Mean Return: 10.436, Mean Wealth (V_T): 118.138, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1660000, Mean Return: 10.421, Mean Wealth (V_T): 117.71, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1670000, Mean Return: 10.405, Mean Wealth (V_T): 117.68, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1680000, Mean Return: 10.452, Mean Wealth (V_T): 118.848, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1690000, Mean Return: 10.364, Mean Wealth (V_T): 116.366, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1700000, Mean Return: 10.382, Mean Wealth (V_T): 117.101, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1710000, Mean Return: 10.405, Mean Wealth (V_T): 117.512, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1720000, Mean Return: 10.404, Mean Wealth (V_T): 117.786, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1730000, Mean Return: 10.436, Mean Wealth (V_T): 118.402, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1740000, Mean Return: 10.397, Mean Wealth (V_T): 117.377, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1750000, Mean Return: 10.397, Mean Wealth (V_T): 117.44, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1760000, Mean Return: 10.41, Mean Wealth (V_T): 117.737, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1770000, Mean Return: 10.414, Mean Wealth (V_T): 117.766, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1780000, Mean Return: 10.376, Mean Wealth (V_T): 116.991, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1790000, Mean Return: 10.408, Mean Wealth (V_T): 117.761, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1800000, Mean Return: 10.379, Mean Wealth (V_T): 117.316, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1810000, Mean Return: 10.314, Mean Wealth (V_T): 115.338, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1820000, Mean Return: 10.356, Mean Wealth (V_T): 116.44, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1830000, Mean Return: 10.447, Mean Wealth (V_T): 118.447, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1840000, Mean Return: 10.387, Mean Wealth (V_T): 117.411, Epsilon: 1, Alpha: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1850000, Mean Return: 10.375, Mean Wealth (V_T): 116.989, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1860000, Mean Return: 10.346, Mean Wealth (V_T): 116.289, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1870000, Mean Return: 10.393, Mean Wealth (V_T): 117.074, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1880000, Mean Return: 10.419, Mean Wealth (V_T): 118.054, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1890000, Mean Return: 10.382, Mean Wealth (V_T): 117.038, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1900000, Mean Return: 10.396, Mean Wealth (V_T): 117.327, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1910000, Mean Return: 10.356, Mean Wealth (V_T): 116.577, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1920000, Mean Return: 10.44, Mean Wealth (V_T): 118.352, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1930000, Mean Return: 10.363, Mean Wealth (V_T): 116.72, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1940000, Mean Return: 10.368, Mean Wealth (V_T): 116.703, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1950000, Mean Return: 10.447, Mean Wealth (V_T): 118.556, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1960000, Mean Return: 10.411, Mean Wealth (V_T): 117.548, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1970000, Mean Return: 10.46, Mean Wealth (V_T): 118.744, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1980000, Mean Return: 10.377, Mean Wealth (V_T): 116.998, Epsilon: 1, Alpha: 0.1\n",
      "Episode: 1990000, Mean Return: 10.402, Mean Wealth (V_T): 117.525, Epsilon: 1, Alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Square root utility function\n",
    "#up_prob, up_ret, down_ret, r, T, dt, V_0, actions, wealth_bins, utility\n",
    "env = BinomialTree(up_prob=4/9, up_ret=1, down_ret=-1/2, r=0, T=2, dt=1, V_0=100, actions=actions, wealth_bins=wealth_bins, utility=\"sqrt\")\n",
    "Q, stats, A = qLearning(env, 2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: (0, 1)\n",
      "State-Action Values:\n",
      "[10.28801086 10.40540089 10.46763096 10.48338078 10.51960108 10.55002203\n",
      " 10.56278248 10.56787654 10.55888306 10.54400638 10.52184846]\n",
      "Best Action (Investment in risky asset): 0.7000000000000001\n",
      "Key: (1, 2)\n",
      "State-Action Values:\n",
      "[10.         10.09607912 10.17139263 10.23699466 10.29150737 10.31086663\n",
      " 10.31390193 10.35880872 10.32129773 10.32270294 10.25538626]\n",
      "Best Action (Investment in risky asset): 0.7000000000000001\n",
      "Key: (2, 2)\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (1, 0)\n",
      "State-Action Values:\n",
      "[10.         10.06233617 10.11227755 10.15905146 10.18777369 10.20913961\n",
      " 10.22473103 10.22286828 10.21062927 10.17626262 10.17510878]\n",
      "Best Action (Investment in risky asset): 0.6000000000000001\n",
      "Key: (2, 0)\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n",
      "Key: (1, 1)\n",
      "State-Action Values:\n",
      "[10.         10.07295485 10.13506459 10.18389907 10.21496375 10.26609202\n",
      " 10.26108235 10.27742746 10.25205478 10.26430861 10.19033664]\n",
      "Best Action (Investment in risky asset): 0.7000000000000001\n",
      "Key: (2, 1)\n",
      "State-Action Values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Best Action (Investment in risky asset): 0.0\n"
     ]
    }
   ],
   "source": [
    "for key in Q.keys():\n",
    "    print(\"Key:\", key)\n",
    "    print(\"State-Action Values:\", Q[key], sep=\"\\n\")\n",
    "    print(\"Best Action (Investment in risky asset):\", decode_action(np.argmax(Q[key]), actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often was each action take in each state\n",
    "for key in A:\n",
    "    print(\"State: {}, Actions: {}\".format(key, A[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_avf(x):\n",
    "    return(4/9 * math.sqrt(2*x*100 + (1-x)*100) + 5/9 * math.sqrt(0.5*x*100 + (1-x)*100))\n",
    "\n",
    "true_Q = np.array([true_avf(x) for x in actions])\n",
    "print(true_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.array([0.0001, 0.001, 0.01, 0.1, 0.2, 0.5])\n",
    "num_episodes   = np.array([100, 1000, 2000, 5000, 10000, 50000, 100000])\n",
    "\n",
    "Q_values = np.zeros((len(num_episodes), len(learning_rates)))\n",
    "for i in range(len(num_episodes)):\n",
    "    for j in range(len(learning_rates)):\n",
    "        episodes = num_episodes[i]\n",
    "        alpha = learning_rates[j]\n",
    "        print(episodes, alpha)\n",
    "        Q, _, _ = qLearning(env, num_episodes = episodes, discount_factor = 1, alpha = alpha, epsilon = 1)\n",
    "        Q_values[i][j] = np.sqrt(np.sum((Q[(0,1)]-true_Q) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_episodes, Q_values.transpose()[0], label=\"Alpha=\"+str(learning_rates[0]))\n",
    "plt.plot(num_episodes, Q_values.transpose()[1], label=\"Alpha=\"+str(learning_rates[1]))\n",
    "plt.plot(num_episodes, Q_values.transpose()[2], label=\"Alpha=\"+str(learning_rates[2]))\n",
    "plt.plot(num_episodes, Q_values.transpose()[3], label=\"Alpha=\"+str(learning_rates[3]))\n",
    "plt.plot(num_episodes, Q_values.transpose()[4], label=\"Alpha=\"+str(learning_rates[4]))\n",
    "plt.plot(num_episodes, Q_values.transpose()[5], label=\"Alpha=\"+str(learning_rates[5]))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"RMS error to true value function\")\n",
    "plt.legend()\n",
    "plt.title(\"Convergence of Q-values for different constant step sizes\")\n",
    "plt.savefig(\"testplot.jpg\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
