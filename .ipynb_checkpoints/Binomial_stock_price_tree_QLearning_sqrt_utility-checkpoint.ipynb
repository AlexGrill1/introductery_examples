{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gym\n",
    "from envs.binomial_tree import BinomialTree, decode_action, encode_action, plot_q_values    # custom BinomialTree dynamics\n",
    "from envs import plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions denote the fraction of wealth invested in the **risky asset**. Actions are discretized with a step size of 10%, i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{A}=[0, 0.1, 0.2, \\dots, 0.9, 1].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions (Investment in risky asset): [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "actions = np.arange(0, 1.01, step=0.1)                  #vector of actions, discrete investment decisions in 10% steps\n",
    "print(\"Actions (Investment in risky asset):\", actions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an instance of the environment. We use the class BinomialTree which simulates the price movements of a risky asset according to an two-step binomial tree with up/down probabilities $p_u = 4/9$ and $p_d=5/9$ and up and down returns $r_u=1$ (i.e. 100%) and $r_d=-1/2$ (i.e. -50%) and riskfree rate $r_f=0$, hence the riskfree rate satisfies\n",
    "$$r_d < r_f < r_u.$$\n",
    "Furthermore, we assume an initial wealth of 100 and a **square root utility function** $U(V_T)=\\sqrt(V_T)$ of terminal wealth $V_T$. The rewards are zero at all time steps before termination and equal to $R_T=\\sqrt{V_T}$ at termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Creating the environment (class BinomalTree)\n",
    "\n",
    "# using U(V_T)=sqrt(V_T)\n",
    "#Inputs: up_prob, up_ret, down_ret, r, T, dt, V_0, actions, utility\n",
    "env = BinomialTree(up_prob=4/9, up_ret=1, down_ret=-0.5, r=0, T=2, dt=1, V_0=100, actions=actions, utility=\"sqrt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States are represented as tuples $(n_t, V_t)$, where $n_t$ denotes the n-th timestep, i.e. if we take $T=5$ and $dt=0.5$ as an example, then the state $s=(0, 100)$ denotes the intial state (i.e. $V_0=100$) and $s=(3, 150)$ denotes the state of having a wealth of 150 at time $t=0+3*dt=1.5$ (i.e. $V_{1.5}=150$).\\\n",
    "Lets sample some steps from the environment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100)\n",
      "((1, 50.0), 0, False, {})\n",
      "((2, 25.0), 5.0, True, {})\n",
      "25.0\n"
     ]
    }
   ],
   "source": [
    "# Simulation of BinomialTree dynamics\n",
    "print(env.reset())      # Reset the environment to state (0, 100)\n",
    "print(env.step(10))     # Take the action 10 (i.e. 90% investment in risky asset) and observe the next state and reward\n",
    "print(env.step(10))     # Take the action 10 (i.e. 90% investment in risky asset) and observe the next state and reward\n",
    "print(env.V_t)          # Prints the current wealth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon-Greedy Policy**\\\n",
    "Source: https://www.geeksforgeeks.org/q-learning-in-python/#:~:text=Q%2DLearning%20is%20a%20basic,defined%20for%20states%20and%20actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "    \"\"\" \n",
    "    Creates an epsilon-greedy policy based \n",
    "    on a given Q-function and epsilon. \n",
    "       \n",
    "    Returns a function that takes the state \n",
    "    as an input and returns the probabilities \n",
    "    for each action in the form of a numpy array  \n",
    "    of length of the action space(set of possible actions). \n",
    "    \"\"\"\n",
    "    def policyFunction(state): \n",
    "   \n",
    "        Action_probabilities = np.ones(num_actions, \n",
    "                dtype = float) * epsilon / num_actions \n",
    "                  \n",
    "        best_action = np.argmax(Q[state]) \n",
    "        Action_probabilities[best_action] += (1.0 - epsilon) \n",
    "        return Action_probabilities \n",
    "   \n",
    "    return policyFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning Algorithm** \\\n",
    "Source: https://www.geeksforgeeks.org/q-learning-in-python/#:~:text=Q%2DLearning%20is%20a%20basic,defined%20for%20states%20and%20actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1, alpha = 1, epsilon = 1): \n",
    "    \"\"\" \n",
    "    Q-Learning algorithm: Off-policy TD control. \n",
    "    Finds the optimal greedy policy while improving \n",
    "    following an epsilon-greedy policy\n",
    "    \"\"\"\n",
    "       \n",
    "    # Action value function \n",
    "    # A nested dictionary that maps \n",
    "    # state -> (action -> action-value). \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) \n",
    "    A = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "   \n",
    "    # Keeps track of useful statistics \n",
    "    stats = plotting.EpisodeStats( \n",
    "        episode_lengths = np.zeros(num_episodes), \n",
    "        episode_rewards = np.zeros(num_episodes))     \n",
    "       \n",
    "    # Create an epsilon greedy policy function \n",
    "    # appropriately for environment action space \n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n) \n",
    "       \n",
    "    # For every episode\n",
    "    returns=np.array([])\n",
    "    terminal_wealths = np.array([])\n",
    "    for ith_episode in range(num_episodes): \n",
    "           \n",
    "        # Reset the environment and pick the first action \n",
    "        state = env.reset() \n",
    "           \n",
    "        for t in itertools.count(): \n",
    "               \n",
    "            # get probabilities of all actions from current state \n",
    "            action_probabilities = policy(state)\n",
    "   \n",
    "            # choose action according to  \n",
    "            # the probability distribution \n",
    "            action = np.random.choice(np.arange( \n",
    "                      len(action_probabilities)), \n",
    "                       p = action_probabilities)\n",
    "            A[state][action] += 1\n",
    "   \n",
    "            # take action and get reward, transit to next state \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "   \n",
    "            # Update statistics \n",
    "            stats.episode_rewards[ith_episode] += reward \n",
    "            stats.episode_lengths[ith_episode] = t\n",
    "               \n",
    "            # TD Update \n",
    "            best_next_action = np.argmax(Q[next_state])     \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += (1/A[state][action]) * td_delta          # Dynamic Learning Rate alpha=1/#visits of state-action pair\n",
    "                                                                         # ensures convergence see Sutton & Barto eq. (2.7)\n",
    "            # done is True if episode terminated    \n",
    "            if done: \n",
    "                returns = np.append(returns, reward)\n",
    "                terminal_wealths=np.append(terminal_wealths, env.V_t)\n",
    "                break\n",
    "                   \n",
    "            state = next_state\n",
    "        \n",
    "        if ith_episode % 10000 == 0:            \n",
    "            print(\"Episode: {}, Mean Return: {}, Mean Wealth (V_T): {}, Epsilon: {}\".format(ith_episode, round(returns.mean(), 3), round(terminal_wealths.mean(), 3), epsilon))\n",
    "            #print(\"td_delta:\", td_delta)\n",
    "            returns = np.array([])\n",
    "            terminal_wealths=np.array([])\n",
    "            \n",
    "        # Epsilon-Decay    \n",
    "        if (ith_episode % 10000 == 0) & (ith_episode != 0):\n",
    "            epsilon *= 0.98\n",
    "            policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "        #    alpha = 0.1\n",
    "        \n",
    "        # Alpha-Decay\n",
    "        #if (ith_episode % 30000 == 0) & (ith_episode != 0):\n",
    "        #    if alpha > 0.0011:\n",
    "        #        alpha *= 1/10\n",
    "            \n",
    "       \n",
    "    return Q, stats, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training our agent..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Mean Return: 6.124, Mean Wealth (V_T): 37.5, Epsilon: 1\n",
      "Episode: 10000, Mean Return: 10.389, Mean Wealth (V_T): 117.402, Epsilon: 1\n",
      "Episode: 20000, Mean Return: 10.395, Mean Wealth (V_T): 117.397, Epsilon: 0.98\n",
      "Episode: 30000, Mean Return: 10.389, Mean Wealth (V_T): 117.032, Epsilon: 0.9603999999999999\n",
      "Episode: 40000, Mean Return: 10.421, Mean Wealth (V_T): 117.884, Epsilon: 0.9411919999999999\n",
      "Episode: 50000, Mean Return: 10.444, Mean Wealth (V_T): 118.653, Epsilon: 0.9223681599999999\n",
      "Episode: 60000, Mean Return: 10.395, Mean Wealth (V_T): 117.399, Epsilon: 0.9039207967999998\n",
      "Episode: 70000, Mean Return: 10.411, Mean Wealth (V_T): 118.189, Epsilon: 0.8858423808639998\n",
      "Episode: 80000, Mean Return: 10.374, Mean Wealth (V_T): 117.091, Epsilon: 0.8681255332467198\n",
      "Episode: 90000, Mean Return: 10.408, Mean Wealth (V_T): 118.17, Epsilon: 0.8507630225817854\n",
      "Episode: 100000, Mean Return: 10.359, Mean Wealth (V_T): 117.145, Epsilon: 0.8337477621301497\n",
      "Episode: 110000, Mean Return: 10.435, Mean Wealth (V_T): 118.675, Epsilon: 0.8170728068875467\n",
      "Episode: 120000, Mean Return: 10.344, Mean Wealth (V_T): 116.18, Epsilon: 0.8007313507497957\n",
      "Episode: 130000, Mean Return: 10.408, Mean Wealth (V_T): 118.084, Epsilon: 0.7847167237347998\n",
      "Episode: 140000, Mean Return: 10.394, Mean Wealth (V_T): 117.496, Epsilon: 0.7690223892601038\n",
      "Episode: 150000, Mean Return: 10.361, Mean Wealth (V_T): 116.93, Epsilon: 0.7536419414749017\n",
      "Episode: 160000, Mean Return: 10.414, Mean Wealth (V_T): 118.135, Epsilon: 0.7385691026454037\n",
      "Episode: 170000, Mean Return: 10.427, Mean Wealth (V_T): 118.984, Epsilon: 0.7237977205924956\n",
      "Episode: 180000, Mean Return: 10.402, Mean Wealth (V_T): 118.625, Epsilon: 0.7093217661806457\n",
      "Episode: 190000, Mean Return: 10.421, Mean Wealth (V_T): 119.568, Epsilon: 0.6951353308570327\n",
      "Episode: 200000, Mean Return: 10.433, Mean Wealth (V_T): 119.913, Epsilon: 0.6812326242398921\n",
      "Episode: 210000, Mean Return: 10.414, Mean Wealth (V_T): 117.928, Epsilon: 0.6676079717550942\n",
      "Episode: 220000, Mean Return: 10.443, Mean Wealth (V_T): 119.647, Epsilon: 0.6542558123199923\n",
      "Episode: 230000, Mean Return: 10.416, Mean Wealth (V_T): 118.936, Epsilon: 0.6411706960735924\n",
      "Episode: 240000, Mean Return: 10.443, Mean Wealth (V_T): 120.95, Epsilon: 0.6283472821521205\n",
      "Episode: 250000, Mean Return: 10.394, Mean Wealth (V_T): 118.721, Epsilon: 0.6157803365090782\n",
      "Episode: 260000, Mean Return: 10.406, Mean Wealth (V_T): 118.422, Epsilon: 0.6034647297788965\n",
      "Episode: 270000, Mean Return: 10.462, Mean Wealth (V_T): 120.064, Epsilon: 0.5913954351833186\n",
      "Episode: 280000, Mean Return: 10.47, Mean Wealth (V_T): 120.766, Epsilon: 0.5795675264796523\n",
      "Episode: 290000, Mean Return: 10.441, Mean Wealth (V_T): 120.535, Epsilon: 0.5679761759500592\n",
      "Episode: 300000, Mean Return: 10.478, Mean Wealth (V_T): 121.732, Epsilon: 0.5566166524310581\n",
      "Episode: 310000, Mean Return: 10.444, Mean Wealth (V_T): 120.165, Epsilon: 0.5454843193824369\n",
      "Episode: 320000, Mean Return: 10.459, Mean Wealth (V_T): 121.49, Epsilon: 0.5345746329947881\n",
      "Episode: 330000, Mean Return: 10.411, Mean Wealth (V_T): 119.07, Epsilon: 0.5238831403348924\n",
      "Episode: 340000, Mean Return: 10.449, Mean Wealth (V_T): 119.56, Epsilon: 0.5134054775281945\n",
      "Episode: 350000, Mean Return: 10.494, Mean Wealth (V_T): 121.497, Epsilon: 0.5031373679776306\n",
      "Episode: 360000, Mean Return: 10.465, Mean Wealth (V_T): 120.449, Epsilon: 0.493074620618078\n",
      "Episode: 370000, Mean Return: 10.462, Mean Wealth (V_T): 121.533, Epsilon: 0.48321312820571644\n",
      "Episode: 380000, Mean Return: 10.391, Mean Wealth (V_T): 119.107, Epsilon: 0.4735488656416021\n",
      "Episode: 390000, Mean Return: 10.395, Mean Wealth (V_T): 119.913, Epsilon: 0.46407788832877006\n",
      "Episode: 400000, Mean Return: 10.451, Mean Wealth (V_T): 121.482, Epsilon: 0.45479633056219465\n",
      "Episode: 410000, Mean Return: 10.45, Mean Wealth (V_T): 121.576, Epsilon: 0.44570040395095073\n",
      "Episode: 420000, Mean Return: 10.48, Mean Wealth (V_T): 123.612, Epsilon: 0.4367863958719317\n",
      "Episode: 430000, Mean Return: 10.426, Mean Wealth (V_T): 118.943, Epsilon: 0.42805066795449304\n",
      "Episode: 440000, Mean Return: 10.482, Mean Wealth (V_T): 120.862, Epsilon: 0.41948965459540316\n",
      "Episode: 450000, Mean Return: 10.463, Mean Wealth (V_T): 122.854, Epsilon: 0.4110998615034951\n",
      "Episode: 460000, Mean Return: 10.49, Mean Wealth (V_T): 121.009, Epsilon: 0.4028778642734252\n",
      "Episode: 470000, Mean Return: 10.45, Mean Wealth (V_T): 120.787, Epsilon: 0.39482030698795667\n",
      "Episode: 480000, Mean Return: 10.488, Mean Wealth (V_T): 121.81, Epsilon: 0.38692390084819756\n",
      "Episode: 490000, Mean Return: 10.481, Mean Wealth (V_T): 121.311, Epsilon: 0.3791854228312336\n",
      "Episode: 500000, Mean Return: 10.477, Mean Wealth (V_T): 120.554, Epsilon: 0.37160171437460887\n",
      "Episode: 510000, Mean Return: 10.499, Mean Wealth (V_T): 123.472, Epsilon: 0.3641696800871167\n",
      "Episode: 520000, Mean Return: 10.43, Mean Wealth (V_T): 122.2, Epsilon: 0.35688628648537435\n",
      "Episode: 530000, Mean Return: 10.503, Mean Wealth (V_T): 121.336, Epsilon: 0.34974856075566685\n",
      "Episode: 540000, Mean Return: 10.435, Mean Wealth (V_T): 119.715, Epsilon: 0.3427535895405535\n",
      "Episode: 550000, Mean Return: 10.538, Mean Wealth (V_T): 124.284, Epsilon: 0.33589851774974244\n",
      "Episode: 560000, Mean Return: 10.445, Mean Wealth (V_T): 122.752, Epsilon: 0.3291805473947476\n",
      "Episode: 570000, Mean Return: 10.519, Mean Wealth (V_T): 123.21, Epsilon: 0.32259693644685267\n",
      "Episode: 580000, Mean Return: 10.449, Mean Wealth (V_T): 122.176, Epsilon: 0.3161449977179156\n",
      "Episode: 590000, Mean Return: 10.426, Mean Wealth (V_T): 120.448, Epsilon: 0.3098220977635573\n",
      "Episode: 600000, Mean Return: 10.466, Mean Wealth (V_T): 120.764, Epsilon: 0.30362565580828615\n",
      "Episode: 610000, Mean Return: 10.551, Mean Wealth (V_T): 122.054, Epsilon: 0.2975531426921204\n",
      "Episode: 620000, Mean Return: 10.51, Mean Wealth (V_T): 121.771, Epsilon: 0.291602079838278\n",
      "Episode: 630000, Mean Return: 10.468, Mean Wealth (V_T): 120.881, Epsilon: 0.2857700382415124\n",
      "Episode: 640000, Mean Return: 10.501, Mean Wealth (V_T): 121.829, Epsilon: 0.2800546374766822\n",
      "Episode: 650000, Mean Return: 10.469, Mean Wealth (V_T): 123.262, Epsilon: 0.27445354472714856\n",
      "Episode: 660000, Mean Return: 10.514, Mean Wealth (V_T): 122.024, Epsilon: 0.2689644738326056\n",
      "Episode: 670000, Mean Return: 10.48, Mean Wealth (V_T): 121.584, Epsilon: 0.26358518435595346\n",
      "Episode: 680000, Mean Return: 10.527, Mean Wealth (V_T): 122.701, Epsilon: 0.25831348066883436\n",
      "Episode: 690000, Mean Return: 10.535, Mean Wealth (V_T): 121.547, Epsilon: 0.2531472110554577\n",
      "Episode: 700000, Mean Return: 10.484, Mean Wealth (V_T): 122.894, Epsilon: 0.24808426683434853\n",
      "Episode: 710000, Mean Return: 10.533, Mean Wealth (V_T): 126.807, Epsilon: 0.24312258149766156\n",
      "Episode: 720000, Mean Return: 10.519, Mean Wealth (V_T): 123.041, Epsilon: 0.2382601298677083\n",
      "Episode: 730000, Mean Return: 10.554, Mean Wealth (V_T): 123.375, Epsilon: 0.23349492727035415\n",
      "Episode: 740000, Mean Return: 10.54, Mean Wealth (V_T): 122.568, Epsilon: 0.22882502872494706\n",
      "Episode: 750000, Mean Return: 10.537, Mean Wealth (V_T): 126.046, Epsilon: 0.22424852815044813\n",
      "Episode: 760000, Mean Return: 10.48, Mean Wealth (V_T): 121.408, Epsilon: 0.21976355758743915\n",
      "Episode: 770000, Mean Return: 10.558, Mean Wealth (V_T): 123.481, Epsilon: 0.21536828643569036\n",
      "Episode: 780000, Mean Return: 10.528, Mean Wealth (V_T): 125.438, Epsilon: 0.21106092070697655\n",
      "Episode: 790000, Mean Return: 10.464, Mean Wealth (V_T): 126.506, Epsilon: 0.20683970229283702\n",
      "Episode: 800000, Mean Return: 10.522, Mean Wealth (V_T): 130.07, Epsilon: 0.20270290824698028\n",
      "Episode: 810000, Mean Return: 10.484, Mean Wealth (V_T): 128.944, Epsilon: 0.19864885008204067\n",
      "Episode: 820000, Mean Return: 10.513, Mean Wealth (V_T): 123.609, Epsilon: 0.19467587308039985\n",
      "Episode: 830000, Mean Return: 10.507, Mean Wealth (V_T): 125.076, Epsilon: 0.19078235561879187\n",
      "Episode: 840000, Mean Return: 10.523, Mean Wealth (V_T): 122.896, Epsilon: 0.18696670850641603\n",
      "Episode: 850000, Mean Return: 10.414, Mean Wealth (V_T): 119.945, Epsilon: 0.18322737433628772\n",
      "Episode: 860000, Mean Return: 10.523, Mean Wealth (V_T): 121.938, Epsilon: 0.17956282684956196\n",
      "Episode: 870000, Mean Return: 10.467, Mean Wealth (V_T): 123.534, Epsilon: 0.1759715703125707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 880000, Mean Return: 10.534, Mean Wealth (V_T): 130.199, Epsilon: 0.1724521389063193\n",
      "Episode: 890000, Mean Return: 10.44, Mean Wealth (V_T): 120.622, Epsilon: 0.16900309612819292\n",
      "Episode: 900000, Mean Return: 10.516, Mean Wealth (V_T): 122.003, Epsilon: 0.16562303420562907\n",
      "Episode: 910000, Mean Return: 10.486, Mean Wealth (V_T): 121.738, Epsilon: 0.16231057352151648\n",
      "Episode: 920000, Mean Return: 10.485, Mean Wealth (V_T): 121.798, Epsilon: 0.15906436205108615\n",
      "Episode: 930000, Mean Return: 10.526, Mean Wealth (V_T): 121.734, Epsilon: 0.15588307481006441\n",
      "Episode: 940000, Mean Return: 10.472, Mean Wealth (V_T): 124.977, Epsilon: 0.15276541331386312\n",
      "Episode: 950000, Mean Return: 10.473, Mean Wealth (V_T): 121.998, Epsilon: 0.14971010504758586\n",
      "Episode: 960000, Mean Return: 10.506, Mean Wealth (V_T): 125.121, Epsilon: 0.14671590294663414\n",
      "Episode: 970000, Mean Return: 10.482, Mean Wealth (V_T): 122.126, Epsilon: 0.14378158488770146\n",
      "Episode: 980000, Mean Return: 10.464, Mean Wealth (V_T): 124.018, Epsilon: 0.14090595318994742\n",
      "Episode: 990000, Mean Return: 10.548, Mean Wealth (V_T): 121.842, Epsilon: 0.13808783412614847\n",
      "Episode: 1000000, Mean Return: 10.577, Mean Wealth (V_T): 123.243, Epsilon: 0.13532607744362551\n",
      "Episode: 1010000, Mean Return: 10.544, Mean Wealth (V_T): 123.359, Epsilon: 0.132619555894753\n",
      "Episode: 1020000, Mean Return: 10.514, Mean Wealth (V_T): 121.008, Epsilon: 0.12996716477685794\n",
      "Episode: 1030000, Mean Return: 10.49, Mean Wealth (V_T): 121.829, Epsilon: 0.12736782148132078\n",
      "Episode: 1040000, Mean Return: 10.564, Mean Wealth (V_T): 123.37, Epsilon: 0.12482046505169436\n",
      "Episode: 1050000, Mean Return: 10.434, Mean Wealth (V_T): 121.308, Epsilon: 0.12232405575066048\n",
      "Episode: 1060000, Mean Return: 10.514, Mean Wealth (V_T): 122.735, Epsilon: 0.11987757463564727\n",
      "Episode: 1070000, Mean Return: 10.437, Mean Wealth (V_T): 119.905, Epsilon: 0.11748002314293432\n",
      "Episode: 1080000, Mean Return: 10.527, Mean Wealth (V_T): 122.097, Epsilon: 0.11513042268007563\n",
      "Episode: 1090000, Mean Return: 10.492, Mean Wealth (V_T): 123.889, Epsilon: 0.11282781422647412\n",
      "Episode: 1100000, Mean Return: 10.528, Mean Wealth (V_T): 123.356, Epsilon: 0.11057125794194463\n",
      "Episode: 1110000, Mean Return: 10.441, Mean Wealth (V_T): 123.479, Epsilon: 0.10835983278310574\n",
      "Episode: 1120000, Mean Return: 10.53, Mean Wealth (V_T): 122.366, Epsilon: 0.10619263612744362\n",
      "Episode: 1130000, Mean Return: 10.538, Mean Wealth (V_T): 123.229, Epsilon: 0.10406878340489474\n",
      "Episode: 1140000, Mean Return: 10.515, Mean Wealth (V_T): 122.338, Epsilon: 0.10198740773679685\n",
      "Episode: 1150000, Mean Return: 10.487, Mean Wealth (V_T): 121.27, Epsilon: 0.09994765958206091\n",
      "Episode: 1160000, Mean Return: 10.526, Mean Wealth (V_T): 121.51, Epsilon: 0.0979487063904197\n",
      "Episode: 1170000, Mean Return: 10.552, Mean Wealth (V_T): 121.461, Epsilon: 0.0959897322626113\n",
      "Episode: 1180000, Mean Return: 10.486, Mean Wealth (V_T): 121.708, Epsilon: 0.09406993761735907\n",
      "Episode: 1190000, Mean Return: 10.57, Mean Wealth (V_T): 124.631, Epsilon: 0.0921885388650119\n",
      "Episode: 1200000, Mean Return: 10.535, Mean Wealth (V_T): 123.616, Epsilon: 0.09034476808771166\n",
      "Episode: 1210000, Mean Return: 10.558, Mean Wealth (V_T): 123.14, Epsilon: 0.08853787272595742\n",
      "Episode: 1220000, Mean Return: 10.553, Mean Wealth (V_T): 125.54, Epsilon: 0.08676711527143827\n",
      "Episode: 1230000, Mean Return: 10.552, Mean Wealth (V_T): 128.062, Epsilon: 0.0850317729660095\n",
      "Episode: 1240000, Mean Return: 10.465, Mean Wealth (V_T): 126.074, Epsilon: 0.08333113750668932\n",
      "Episode: 1250000, Mean Return: 10.594, Mean Wealth (V_T): 129.706, Epsilon: 0.08166451475655553\n",
      "Episode: 1260000, Mean Return: 10.497, Mean Wealth (V_T): 126.88, Epsilon: 0.08003122446142442\n",
      "Episode: 1270000, Mean Return: 10.448, Mean Wealth (V_T): 123.466, Epsilon: 0.07843059997219594\n",
      "Episode: 1280000, Mean Return: 10.514, Mean Wealth (V_T): 121.711, Epsilon: 0.07686198797275202\n",
      "Episode: 1290000, Mean Return: 10.461, Mean Wealth (V_T): 121.187, Epsilon: 0.07532474821329699\n",
      "Episode: 1300000, Mean Return: 10.511, Mean Wealth (V_T): 121.402, Epsilon: 0.07381825324903105\n",
      "Episode: 1310000, Mean Return: 10.467, Mean Wealth (V_T): 121.96, Epsilon: 0.07234188818405042\n"
     ]
    }
   ],
   "source": [
    "# Training of the Agent\n",
    "num_episodes = 2500000                     # Training for 2.5 mio. Episodes\n",
    "Q, stats, A = qLearning(env, num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the returns have a high variance, we cannot really see an clear improvement in the following plots, but with the parameters specified as above we know that the agents learned the optimal investment decision of $a_t = 0.7\\, \\forall t$ if we achieve a mean wealth of around 124."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the Q-values for each state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Q.keys():\n",
    "    print(\"Key:\", key)\n",
    "    print(\"State-Action Values:\", Q[key], sep=\"\\n\")\n",
    "    print(\"Best Action (Investment in risky asset):\", decode_action(np.argmax(Q[key]), actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often was each action take in each state\n",
    "for key, value in A.items():\n",
    "    print(\"State: {}, Actions: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the Q-value surface for each state-action pair. The predicted optimal actions $argmax_a Q(s,a)$ are indicated as red dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_q_values(Q, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Q-values seem to be reasonable, as the surface is smooth. The predicted actions take values in the set $\\{0.5, 0.6, 0.7, 0.8, 0.9, 1\\}$ and most predicted actions seem to be around $0.7$. Still we can observe that even for this small example and training for 2.5m episodes the Q-Learning cannot figure out the theoretical optimal strategy, which is $a_t=0.7\\,\\, \\forall s \\in \\mathcal{S}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training another Agent for only 5000 episodes\n",
    "Q2, _, _ = qLearning(env, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_q_values(Q2, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that after 5000 episodes the Q-value surface is still very rigid. Hence, it seems that the Q-values have not converged yet, and longer training is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assessing the convergence of the Q-value function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
